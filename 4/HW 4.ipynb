{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aleksandr.khvorov/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymorphy2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from catboost import CatBoost, CatBoostRegressor, Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem = Mystem() \n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    tokens = mystem.lemmatize(text.lower())\n",
    "    tokens = [token for token in tokens if token not in russian_stopwords\\\n",
    "              and token != \" \" \\\n",
    "              and token.strip() not in punctuation]\n",
    "    \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def get_data():\n",
    "    data = []\n",
    "    q_ids = []\n",
    "    with open(\"train_qa.csv\") as f:\n",
    "        for line in list(f)[1:]:\n",
    "            tokens = line.strip().split('\",\"')\n",
    "            data.append(tuple(map(lambda x: list(filter(lambda w: w != \" \" and w.strip() not in punctuation, \n",
    "                                            word_tokenize(x.strip(' \"')))), tokens[2:])))\n",
    "            q_ids.append(int(tokens[1]))\n",
    "    return data, q_ids\n",
    "            \n",
    "data, q_ids = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50364"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_train, data_test, _, _ = train_test_split(data, [0] * len(data), train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 8223,\n",
       "         2: 11170,\n",
       "         3: 10785,\n",
       "         4: 7057,\n",
       "         5: 4495,\n",
       "         6: 2939,\n",
       "         7: 2131,\n",
       "         8: 1389,\n",
       "         9: 764})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(sorted([len(x[2]) for x in data if len(x[2]) < 10]))\n",
    "# np.mean(sorted([len(x[2]) for x in data if len(x[2]) < 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['в', 'Древнем', 'Египте']\n",
      "['в', 'Древнем', 'Египте']\n",
      "\n",
      "['COSTAR']\n",
      "['Телескоп', 'имеет', 'модульную']\n",
      "\n",
      "['теория', 'дрейфа', 'материков']\n",
      "['Альфреда', 'маргинальной', 'науки', 'и']\n",
      "\n",
      "['изделиям', 'из', 'монолитных', 'камней']\n",
      "['различных', 'фракций', 'для']\n",
      "\n",
      "['оральные', 'и', 'назальные']\n",
      "['встречаются', 'в', 'слабым', 'гласным']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def avg_position_of_question(text, question):\n",
    "    ind_sum = 0\n",
    "    ind_count = 0\n",
    "    inds = []\n",
    "    for word in question:\n",
    "#         print(word)\n",
    "        ind = 0\n",
    "        while word in text[ind:]:\n",
    "#             print(ind, word)\n",
    "            ind = text.index(word, ind)\n",
    "            inds.append(ind)\n",
    "            ind_sum += ind\n",
    "            ind_count += 1\n",
    "            ind += 1\n",
    "#     return ind_sum // ind_count\n",
    "    if len(inds) == 0:\n",
    "        return 0\n",
    "    return inds[len(inds) // 2]\n",
    "\n",
    "def window_around_question_with_width(text, question, width=3):\n",
    "    position = avg_position_of_question(text, question)\n",
    "    start_width = len(question) // 2\n",
    "#     print(text)\n",
    "#     print(question)\n",
    "#     print(\"Position:\", position, \"Start width:\", start_width)\n",
    "    for i in range(1, 4 * width):\n",
    "        first = text[max(0, position - start_width - i) : max(0, position - start_width)]\n",
    "        second = text[min(position + start_width, len(text)) : min(position + start_width + i, len(text))]\n",
    "        sent = first + second\n",
    "#         print(start_width, i)\n",
    "#         print(\"First\", first)\n",
    "#         print(second)\n",
    "        ans = []\n",
    "        for w in sent:\n",
    "            if w not in question and w not in punctuation:\n",
    "                ans.append(w)\n",
    "#         print(ans)\n",
    "#         print()\n",
    "        if len(ans) >= width:\n",
    "            return ans\n",
    "        \n",
    "# window_around_question_with_width(data[0][0], data[0][1])\n",
    "for i in range(5):\n",
    "#     print(data[i][0])\n",
    "#     print(data[i][1])\n",
    "    print(data[i][2])\n",
    "    print(window_around_question_with_width(data[i][0], data[i][1]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file():\n",
    "    data = []\n",
    "    w = open(\"ans.txt\", 'w')\n",
    "    with open(\"dataset_281937_1.txt\") as f:\n",
    "        for line in list(f)[1:]:\n",
    "            tokens = line.strip().split('\\t')\n",
    "            text, q = map(lambda x: word_tokenize(x.strip(' \"')), tokens[2:])\n",
    "            ans = window_around_question_with_width(text, q) or []\n",
    "            w.write(tokens[1] + \"\\t\" + \" \".join(ans) + \"\\n\")\n",
    "            \n",
    "process_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, qs, anss = {}, {}, {}\n",
    "norm_words = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dicts(data, q_ids):\n",
    "    for d, q_id in zip(data, q_ids):\n",
    "        if len(d) == 3:\n",
    "            text, q, ans = d\n",
    "        else:\n",
    "            text, q = d\n",
    "        texts[q_id] = text\n",
    "        qs[q_id] = q\n",
    "        for word in text + q:\n",
    "            if word not in norm_words:\n",
    "                w = morph.parse('стали')[0].normal_form\n",
    "                norm_words[word] = w\n",
    "        if len(d) == 3:\n",
    "            anss[q_id] = ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_dicts(data, q_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Point:\n",
    "    def __init__(self, q_id, sample_ind, sample_len):\n",
    "        self.q_id = q_id\n",
    "        self.sample_ind = sample_ind\n",
    "        self.sample_len = sample_len\n",
    "        self._answer = None\n",
    "        self._norm_text = None\n",
    "        self._norm_q = None\n",
    "        self._norm_ans = None\n",
    "    \n",
    "    def text(self):\n",
    "        return texts[self.q_id]\n",
    "    \n",
    "    def question(self):\n",
    "        return qs[self.q_id]\n",
    "        \n",
    "    def answer(self):\n",
    "        if self._answer is None:\n",
    "            self._answer = self.text()[self.sample_ind : self.sample_ind + self.sample_len]\n",
    "        return self._answer\n",
    "    \n",
    "    def norm_text(self):\n",
    "        if self._norm_text is None:\n",
    "            self._norm_text = [norm_words[w] for w in self.text()]\n",
    "        return self._norm_text\n",
    "    \n",
    "    def norm_q(self):\n",
    "        if self._norm_q is None:\n",
    "            self._norm_q = [norm_words[w] for w in self.question()]\n",
    "        return self._norm_q\n",
    "    \n",
    "    def norm_ans(self):\n",
    "        if self._norm_ans is None:\n",
    "            self._norm_ans = [norm_words[w] for w in self.answer()]\n",
    "        return self._norm_ans\n",
    "        \n",
    "\n",
    "def samples_from_example(text, question, answer, q_id, ranges=[2, 3]):\n",
    "    samples = []\n",
    "    ans = []\n",
    "    true_ind = 0\n",
    "    true_ans_len = len(answer)\n",
    "    for i in range(len(text) - true_ans_len):\n",
    "        if text[i : i + true_ans_len] == answer:\n",
    "            true_ind = i\n",
    "            break\n",
    "    for ans_len in ranges:\n",
    "        for i in range(len(text) - ans_len):\n",
    "            samples.append(Point(q_id, i, ans_len))\n",
    "            \n",
    "            def target1():\n",
    "                return min(abs(true_ind - i), true_ans_len) / max(ans_len, true_ans_len)\n",
    "            \n",
    "            def target2():\n",
    "                l = max(ans_len, true_ans_len)\n",
    "                intersect_len = max(0, (abs(true_ind - i) - l))\n",
    "                return intersect_len / l\n",
    "            \n",
    "            def target3():\n",
    "                return len(set(answer) & set(text[i : i + ans_len])) / true_ans_len\n",
    "            \n",
    "            def target4():\n",
    "                ans = text[i : i + ans_len]\n",
    "                precision = len([for w in answer if w in ans]) / ans_len\n",
    "                recall = len([for w in ans if w in answer]) / true_ans_len\n",
    "                return 2 * precision * recall / (precision + recall)\n",
    "            \n",
    "            ans.append(target4())\n",
    "    return samples, ans\n",
    "\n",
    "def create_dataset(data, q_ids, ranges=[2, 3]):\n",
    "    X, y = [], []\n",
    "    new_q_ids = []\n",
    "    for p, q_id in zip(data, q_ids):\n",
    "        samples, ans = samples_from_example(p[0], p[1], p[2], q_id, ranges)\n",
    "        X += samples\n",
    "        y += ans\n",
    "        new_q_ids += [q_id] * len(ans)\n",
    "    return X, y, new_q_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text, question, sample_ind, sample_len\n",
    "\n",
    "def intersected_words_num(point):\n",
    "    intersect_words_num = len(set(point.question()) & set(point.answer()))\n",
    "    return [intersect_words_num, intersect_words_num / len(point.question()), intersect_words_num / point.sample_len]\n",
    "\n",
    "def intersected_norm_words_num(point):\n",
    "    intersect_words_num = len(set(point.norm_q()) & set(point.norm_ans()))\n",
    "    return [intersect_words_num, intersect_words_num / len(point.question()), intersect_words_num / point.sample_len]\n",
    "    \n",
    "def stop_words_in_answer_ratio(point):\n",
    "    stop_num = len([w for w in point.norm_ans() if w in russian_stopwords])\n",
    "    return [stop_num, stop_num / point.sample_len]\n",
    "\n",
    "def query_len(point):\n",
    "    return [len(point.question())]\n",
    "\n",
    "def answer_len(point):\n",
    "    return [point.sample_len]\n",
    "\n",
    "def position_dist(p):\n",
    "    q_pos = avg_position_of_question(p.norm_text(), p.norm_q())\n",
    "    return [abs(q_pos - p.sample_ind)]\n",
    "\n",
    "def answer_punctuation(p):\n",
    "    n = len([w for w in p.answer() if w in punctuation])\n",
    "    return [n, n / p.sample_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(points):\n",
    "    features_list = [intersected_words_num, intersected_norm_words_num, stop_words_in_answer_ratio,\n",
    "                     query_len, answer_len, position_dist, answer_punctuation]\n",
    "    data = []\n",
    "    for p in points:\n",
    "        x = []\n",
    "        for f in features_list:\n",
    "            x += f(p)\n",
    "        data.append(x)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, q_id_tr, q_id_te = train_test_split(data, q_ids, train_size=0.2)\n",
    "ranges = [3]\n",
    "points_tr, y_tr, q_id_tr = create_dataset(data_train, q_id_tr, ranges=ranges)\n",
    "# points_te, y_te, q_id_te = create_dataset(data_test, q_id_te, ranges=ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data[0])\n",
    "# pss, yss, qss = create_dataset([data[0]], [q_ids[0]], ranges=[3])\n",
    "# for i in range(len(pss)):\n",
    "#     print(pss[i].answer())\n",
    "#     print(yss[i])\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = features(points_tr)\n",
    "# X_te = features(points_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = Pool(\n",
    "    data=X_tr,\n",
    "    label=y_tr,\n",
    "    group_id=q_id_tr\n",
    ")\n",
    "\n",
    "# test = Pool(\n",
    "#     data=X_te,\n",
    "#     label=y_te,\n",
    "#     group_id=q_id_te\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6904709\ttotal: 542ms\tremaining: 1m 47s\n",
      "25:\tlearn: 0.6572281\ttotal: 15.4s\tremaining: 1m 42s\n",
      "50:\tlearn: 0.6501086\ttotal: 31.6s\tremaining: 1m 32s\n",
      "75:\tlearn: 0.6482070\ttotal: 47.2s\tremaining: 1m 16s\n",
      "100:\tlearn: 0.6473885\ttotal: 1m 4s\tremaining: 1m 2s\n",
      "125:\tlearn: 0.6470456\ttotal: 1m 19s\tremaining: 46.9s\n",
      "150:\tlearn: 0.6467218\ttotal: 1m 36s\tremaining: 31.3s\n",
      "175:\tlearn: 0.6465335\ttotal: 1m 52s\tremaining: 15.3s\n",
      "199:\tlearn: 0.6463876\ttotal: 2m 7s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoost at 0x1a93c1d8d0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CatBoost({'loss_function': 'PairLogit:max_pairs=1000', 'iterations': 200, 'metric_period': 25, 'random_seed': 0})\n",
    "model.fit(train, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.1502644\ttotal: 121ms\tremaining: 24s\n",
      "25:\tlearn: 0.1490274\ttotal: 1.79s\tremaining: 12s\n",
      "50:\tlearn: 0.1487333\ttotal: 3.3s\tremaining: 9.64s\n",
      "75:\tlearn: 0.1486537\ttotal: 4.72s\tremaining: 7.71s\n",
      "100:\tlearn: 0.1486254\ttotal: 6.14s\tremaining: 6.02s\n",
      "125:\tlearn: 0.1486081\ttotal: 7.53s\tremaining: 4.42s\n",
      "150:\tlearn: 0.1485968\ttotal: 8.9s\tremaining: 2.89s\n",
      "175:\tlearn: 0.1485853\ttotal: 10.3s\tremaining: 1.41s\n",
      "199:\tlearn: 0.1485757\ttotal: 11.7s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x1a2df82a90>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_reg = CatBoostRegressor(iterations=200, metric_period=25, random_seed=0)\n",
    "model_reg.fit(train, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([32.06656718,  8.4165228 , 42.65196945,  0.        ,  3.34250813,\n",
       "        0.        ,  0.        ,  0.        ,  3.02541846,  0.        ,\n",
       "       10.49701398,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_reg.get_feature_importance(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_eval_file():\n",
    "    data = []\n",
    "    q_ids = []\n",
    "    with open(\"dataset_281937_1.txt\") as f:\n",
    "        for line in list(f)[1:]:\n",
    "            tokens = line.strip().split('\\t')\n",
    "            text, q = map(lambda x: word_tokenize(x.strip(' \"')), tokens[2:])\n",
    "            data.append((text, q, ['привет']))\n",
    "            q_id = int(tokens[1])\n",
    "            q_ids.append(q_id)\n",
    "    return data, q_ids\n",
    "            \n",
    "eval_data, eval_q_ids = read_eval_file()\n",
    "update_dicts(eval_data, eval_q_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_eval, y_eval, q_id_eval = create_dataset(eval_data, eval_q_ids, ranges=ranges)\n",
    "X_eval = features(points_eval)\n",
    "eval_pool = Pool(data=X_eval, group_id=q_id_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_reg.predict(eval_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ans_from_groups(y_pred, q_id_eval):\n",
    "    q_id_ans = {}\n",
    "    for i, (y, q_id) in enumerate(zip(y_pred, q_id_eval)):\n",
    "        if q_id not in q_id_ans:\n",
    "            q_id_ans[q_id] = i\n",
    "        if y > y_pred[q_id_ans[q_id]]:\n",
    "            q_id_ans[q_id] = i\n",
    "    return q_id_ans\n",
    "\n",
    "def write_ans(points, q_id_ans, eval_q_ids):\n",
    "    w = open(\"ans.txt\", 'w')\n",
    "    for q_id in eval_q_ids:\n",
    "        p = points[q_id_ans[q_id]]\n",
    "        ans = p.answer()\n",
    "        w.write(str(q_id) + \"\\t\" + \" \".join(ans) + \"\\n\")\n",
    "        \n",
    "write_ans(points_eval, ans_from_groups(y_pred, q_id_eval), eval_q_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
