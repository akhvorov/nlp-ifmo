{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymorphy2\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from catboost import CatBoostClassifier, Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_to_words(text: str):\n",
    "    start = 0\n",
    "    end = 0\n",
    "    sent = []\n",
    "    for word in text.split(' '):\n",
    "        end += len(word) + 1\n",
    "        i, j = 0, 1\n",
    "        while i < len(word) and not word[i].isalpha():\n",
    "            i += 1\n",
    "        while j < len(word) and not word[-j].isalpha():\n",
    "            j += 1\n",
    "        wword = word[i:]\n",
    "        j -= 1\n",
    "        if j > 0:\n",
    "            wword = wword[:-j]\n",
    "        if wword:\n",
    "            sent.append((wword, start + i, len(wword)))\n",
    "        start = end\n",
    "    return sent\n",
    "\n",
    "def read_data(str_data) -> List[List[str]]:\n",
    "    return list(map(lambda s: tokenize_to_words(s), str_data.split('\\n')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset = []\n",
    "words_ners = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_by_annoteted(file_name, words, ann_words):\n",
    "    ann_words[file_name] = []\n",
    "    for ann in open(\"collection5/\" + file_name + \".ann\"):\n",
    "        anns = ann.split('\\t')\n",
    "        word = anns[2].strip()\n",
    "        w = morph.parse(word)[0].normal_form\n",
    "        ner = anns[1].split(' ')[0]\n",
    "        ner_map = {'PER': 'PERSON', 'MEDIA': 'ORG'}\n",
    "        if ner in ner_map:\n",
    "            ner = ner_map[ner]\n",
    "        if ner in ['PERSON', 'ORG']:\n",
    "#                     words[w] = ner\n",
    "            if w not in words:\n",
    "                words[w] = {}\n",
    "            if word not in words:\n",
    "                words[word] = {}\n",
    "            ann_words[file_name].append(word)\n",
    "            words[w][ner] = words[w].get(ner, 0) + 1\n",
    "            words[word][ner] = words[word].get(ner, 0) + 1\n",
    "            ner_dataset.append((word, ner))\n",
    "            \n",
    "def update_by_text(file_name, words, ann_words):\n",
    "    for line in open(\"collection5/\" + file_name + \".txt\"):\n",
    "        for word, i, j in tokenize_to_words(line):\n",
    "            if word not in ann_words[file_name]:\n",
    "                w = morph.parse(word)[0].normal_form\n",
    "                if w not in words:\n",
    "                    words[w] = {}\n",
    "                if word not in words:\n",
    "                    words[word] = {}\n",
    "                words[w]['NONE'] = words[w].get('NONE', 0) + 1\n",
    "                words[word]['NONE'] = words[word].get('NONE', 0) + 1\n",
    "                ner_dataset.append((word, 'NONE'))\n",
    "\n",
    "def collection5(words):\n",
    "    ann_words = {}\n",
    "    txt_words = {}\n",
    "    for file in os.listdir(\"collection5\"):\n",
    "        file_name = file[:-4]\n",
    "        if file[-3:] == \"ann\":\n",
    "            update_by_annoteted(file_name, words, ann_words)\n",
    "        elif file[-3:] == \"txt\":\n",
    "            if file_name not in ann_words:\n",
    "                update_by_annoteted(file_name, words, ann_words)\n",
    "            txt_words[file_name] = []\n",
    "            update_by_text(file_name, words, ann_words)  \n",
    "    return words\n",
    "            \n",
    "words_ners = collection5(words_ners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57685"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_ners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_names(words):\n",
    "    for file in os.listdir(\"names\"):\n",
    "        if file[-1] != 'v':\n",
    "            continue\n",
    "        for line in list(open(\"names/\" + file))[1:]:\n",
    "            name = line.split(';')[1]\n",
    "            if name not in words:\n",
    "                words[name] = {}\n",
    "            words[name]['PERSON'] = words[name].get('PERSON', 0) + 1\n",
    "    return words\n",
    "            \n",
    "words_ners = all_names(words_ners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "440513"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_ners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data1(words):\n",
    "    for text, ann in zip(open(\"train_sentences.txt\"), open(\"train_nes.txt\")):\n",
    "        all_words = set(text.strip().split(' '))\n",
    "        seen_words = set()\n",
    "        anns = ann.split(' ')[:-1]\n",
    "        sent = tokenize_to_words(text)\n",
    "        for i in range(0, len(anns), 3):\n",
    "            start = int(anns[i])\n",
    "            finish = start + int(anns[i + 1])\n",
    "            word = text[start:finish]\n",
    "            seen_words.add(word)\n",
    "            word = morph.parse(word.strip())[0].normal_form\n",
    "            ner = anns[2]\n",
    "            if word not in words:\n",
    "                words[word] = {}\n",
    "            words[word][ner] = words[word].get(ner, 0) + 1\n",
    "            ner_dataset.append((word, ner))\n",
    "        unseen_words = all_words - seen_words\n",
    "        for word in unseen_words:\n",
    "            if word not in words:\n",
    "                words[word] = {}\n",
    "            words[word]['NONE'] = words[word].get('NONE', 0) + 1\n",
    "            ner_dataset.append((word, 'NONE'))\n",
    "    return words\n",
    "            \n",
    "words_ners = get_data1(words_ners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "450384"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_ners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data2(words):\n",
    "    for text in open(\"train_sentences_enhanced.txt\"):\n",
    "        for word in text.split(' '):\n",
    "            final_ner = 'NONE'\n",
    "            if word and word[-1] == '}' and '{' in word:\n",
    "                start = word.index('{')\n",
    "                ner = word[start + 1 : -1]\n",
    "                w = word[:start]\n",
    "                i = 0\n",
    "                while i < len(w) and not w[i].isalpha():\n",
    "                    i += 1\n",
    "                w = w[i:]\n",
    "                w = morph.parse(w.strip())[0].normal_form\n",
    "                if ner in ['PERSON', 'ORG']:\n",
    "                    final_ner = ner\n",
    "            if word not in words:\n",
    "                words[word] = {}\n",
    "            words[word][final_ner] = words[word].get(final_ner, 0) + 1\n",
    "            ner_dataset.append((word, final_ner))\n",
    "    return words\n",
    "            \n",
    "words_ners = get_data2(words_ners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "457543"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_ners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "игнатьев {'PERSON': 14, 'NONE': 9}\n",
      "Игнатьев {'PERSON': 15, 'NONE': 5}\n",
      "минфин {'ORG': 69, 'NONE': 4}\n",
      "Минфином {'ORG': 4}\n",
      "цб {'ORG': 99, 'NONE': 2}\n",
      "ЦБ {'ORG': 98, 'NONE': 2}\n",
      "сергей игнатьев {'PERSON': 16}\n",
      "Сергей Игнатьев {'PERSON': 16}\n",
      "госдума {'ORG': 288, 'NONE': 3, 'PERSON': 1}\n",
      "Госдуме {'ORG': 44, 'NONE': 1}\n"
     ]
    }
   ],
   "source": [
    "for k in list(words_ners.keys())[:10]:\n",
    "    print(k, words_ners[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arg_max_dict(d):\n",
    "    max_v = 0\n",
    "    max_a = None\n",
    "    for k in d:\n",
    "        if max_v < d[k]:\n",
    "            max_v = d[k]\n",
    "            max_a = k\n",
    "    return max_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Список', 0, 6),\n",
       "  ('пяти', 7, 4),\n",
       "  ('ключевых', 12, 8),\n",
       "  ('тем', 21, 3),\n",
       "  ('саммита', 25, 7),\n",
       "  ('был', 33, 3),\n",
       "  ('обнародован', 37, 11),\n",
       "  ('на', 49, 2),\n",
       "  ('минувшей', 52, 8),\n",
       "  ('неделе', 61, 6)],\n",
       " [('После', 0, 5),\n",
       "  ('первой', 6, 6),\n",
       "  ('инспекции', 13, 9),\n",
       "  ('предложенного', 23, 13),\n",
       "  ('для', 37, 3),\n",
       "  ('Конституционного', 41, 16),\n",
       "  ('суда', 58, 4),\n",
       "  ('здания', 63, 6),\n",
       "  ('из', 71, 2),\n",
       "  ('которого', 74, 8),\n",
       "  ('в', 83, 1),\n",
       "  ('настоящий', 85, 9),\n",
       "  ('момент', 95, 6),\n",
       "  ('переезжает', 102, 10),\n",
       "  ('Исторический', 113, 12),\n",
       "  ('государственный', 126, 15),\n",
       "  ('архив', 142, 5),\n",
       "  ('в', 149, 1),\n",
       "  ('конце', 151, 5),\n",
       "  ('года', 162, 4),\n",
       "  ('Зорькин', 167, 7),\n",
       "  ('высказывался', 175, 12),\n",
       "  ('о', 188, 1),\n",
       "  ('проекте', 190, 7),\n",
       "  ('положительно', 198, 12)]]"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_data = open(\"dataset_40163_1.txt\").read().strip()\n",
    "# str_data = \"\"\"Барак Обама принимает в Белом доме своего французского коллегу Николя Саркози.\n",
    "# О возможном включении благотворительного фонда в список \"иностранных агентов\" 7 мая написала газета «Ведомости».\"\"\"\n",
    "data = read_data(str_data)\n",
    "data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'По словам Н.Прянишникова, прежде всего должна быть представлена инновационная идея, которая имеет потенциал развития.'"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_data.split('\\n')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors: 789 / 4178\n",
      "3389 3389\n"
     ]
    }
   ],
   "source": [
    "all_words = set()\n",
    "for sent in data:\n",
    "    for x, i, j in sent:\n",
    "        all_words.add(x)\n",
    "all_words = list(all_words)\n",
    "words_features, selected_words = features(all_words, all_words)\n",
    "words_ind = {w: i for i, w in enumerate(selected_words)}\n",
    "print(len(words_features), len(words_ind))\n",
    "words_features = change_nones(words_features)\n",
    "words_pred_ner = full_model.predict(words_features).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EOL\n",
      "167 7 PERSON EOL\n",
      "47 10 ORG EOL\n",
      "16 11 ORG EOL\n",
      "121 6 PERSON 154 3 ORG EOL\n"
     ]
    }
   ],
   "source": [
    "def write_result(data):\n",
    "    res = []\n",
    "    for words in data:\n",
    "        sent = []\n",
    "        for (word, start, l) in words:\n",
    "            normal = morph.parse(word)[0].normal_form\n",
    "            ner = None\n",
    "            if word in words_ners:\n",
    "                ner = arg_max_dict(words_ners[word])\n",
    "            if (ner is None or ner == 'NONE') and normal in words_ners:\n",
    "                ner = arg_max_dict(words_ners[normal])\n",
    "#                 print(\"col5\")\n",
    "#             if ner is None and word in names:\n",
    "#                 ner = names[word]\n",
    "#                 print(\"names\")\n",
    "#             if ner is None and normal in train_data1:\n",
    "#                 ner = train_data1[normal]\n",
    "#                 print(normal, ner)\n",
    "#                 print(1)\n",
    "#             if ner is None and normal in train_data2:\n",
    "#                 ner = train_data2[normal]\n",
    "#                 print(2)\n",
    "#             if (ner is None or ner == 'NONE') and word in words_ind:\n",
    "#                 ner = int_to_ner[words_pred_ner[words_ind[word]]]\n",
    "            if ner is not None and ner != 'NONE':\n",
    "                sent.append(f\"{start} {l} {ner}\")\n",
    "        sent.append(\"EOL\")\n",
    "        res.append(\" \".join(sent))\n",
    "    print(\"\\n\".join(res[:5]))\n",
    "    with open(\"out.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(res))\n",
    "        \n",
    "write_result(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 18707, 1: 17518, 2: 270597}\n"
     ]
    }
   ],
   "source": [
    "ner_to_int = {'PERSON': 0, 'ORG': 1, 'NONE': 2}\n",
    "int_to_ner = {0: 'PERSON', 1: 'ORG', 2: None}\n",
    "ner_words = [x[0] for x in ner_dataset]\n",
    "ner_target = np.array([ner_to_int[x[1]] for x in ner_dataset])\n",
    "target = {}\n",
    "for yi in ner_target:\n",
    "    if yi not in target:\n",
    "        target[yi] = 0\n",
    "    target[yi] += 1\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors: 9768 / 306822\n"
     ]
    }
   ],
   "source": [
    "def first_is_capital(word):\n",
    "    return [word[0] != word[0].lower()]\n",
    "\n",
    "def words_ner_dict(word):\n",
    "    wn = words_ners.get(word, {})\n",
    "    summa = sum(wn.values())\n",
    "    return [wn.get(k, 0.) / summa for k in ner_to_int]\n",
    "\n",
    "def norm_words_ner_dict(word):\n",
    "    w = morph.parse(word)[0].normal_form\n",
    "    wn = words_ners.get(w, {})\n",
    "    summa = sum(wn.values())\n",
    "    return [wn.get(k, 0.) / summa for k in ner_to_int]\n",
    "\n",
    "def pymorphy(word):\n",
    "    p = morph.parse(word)[0]\n",
    "    return [p.tag.POS,\n",
    "            p.tag.animacy,\n",
    "            p.tag.aspect,\n",
    "            p.tag.case,\n",
    "            p.tag.gender,\n",
    "            p.tag.involvement,\n",
    "            p.tag.mood,\n",
    "            p.tag.number,\n",
    "            p.tag.person,\n",
    "            p.tag.tense,\n",
    "            p.tag.transitivity,\n",
    "            p.tag.voice]\n",
    "\n",
    "def features(X, y):\n",
    "    data = []\n",
    "    ys = []\n",
    "    errs = 0\n",
    "    for x, yi in zip(X, y):\n",
    "        try:\n",
    "            datum = []\n",
    "            for f in [first_is_capital, pymorphy, words_ner_dict, norm_words_ner_dict]:\n",
    "                datum += f(x)\n",
    "            data.append(datum)\n",
    "            ys.append(yi)\n",
    "        except Exception:\n",
    "            errs += 1\n",
    "    print(f\"Errors: {errs} / {len(y)}\")\n",
    "    return data, ys\n",
    "\n",
    "# print(len(ner_words))\n",
    "orig_X, y = features(ner_words, ner_target)\n",
    "# print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_float(X):\n",
    "    coder = {}\n",
    "    X_n = []\n",
    "    for xs in X:\n",
    "        xs_n = []\n",
    "        for i, x in enumerate(xs):\n",
    "            if isinstance(x, float):\n",
    "                xs_n.append(x)\n",
    "            else:\n",
    "                if i not in coder:\n",
    "                    coder[i] = {}\n",
    "                coder[i][x] = len(coder[i])\n",
    "                xs_n.append(coder[i][x])\n",
    "        X_n.append(xs_n)\n",
    "    return X_n\n",
    "\n",
    "def balance_target(X, y):\n",
    "    X_n, y_n = [], []\n",
    "    n = 0\n",
    "    for xs, yi in zip(X, y):\n",
    "        if yi == 2:\n",
    "            n = n + 1\n",
    "            if n % 2 == 0:\n",
    "                X_n.append(xs)\n",
    "                y_n.append(yi)\n",
    "        else:\n",
    "            X_n.append(xs)\n",
    "            y_n.append(yi)\n",
    "    return X_n, y_n\n",
    "\n",
    "# coded_X = to_float(orig_X)\n",
    "balanced_X, balanced_y = balance_target(coded_X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 17444, 1: 15660, 2: 131975}\n"
     ]
    }
   ],
   "source": [
    "target = {}\n",
    "for yi in balanced_y:\n",
    "    if yi not in target:\n",
    "        target[yi] = 0\n",
    "    target[yi] += 1\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 18, 3, 3, 10, 4, 2, 3, 3, 4, 4, 3, 3, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [2, 18, 3, 3, 10, 4, 2, 3, 3, 4, 4, 3, 3, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0]]\n",
      "[1, 2]\n"
     ]
    }
   ],
   "source": [
    "X_tr, X_te, y_tr, y_te = train_test_split(balanced_X, balanced_y)\n",
    "print(X_tr[:2])\n",
    "print(y_tr[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.0419455\ttest: 1.0419675\tbest: 1.0419675 (0)\ttotal: 187ms\tremaining: 55.9s\n",
      "25:\tlearn: 0.3871019\ttest: 0.3875390\tbest: 0.3875390 (25)\ttotal: 3.73s\tremaining: 39.3s\n",
      "50:\tlearn: 0.1881264\ttest: 0.1888965\tbest: 0.1888965 (50)\ttotal: 7.61s\tremaining: 37.2s\n",
      "75:\tlearn: 0.1069587\ttest: 0.1080567\tbest: 0.1080567 (75)\ttotal: 11.8s\tremaining: 34.9s\n",
      "100:\tlearn: 0.0705625\ttest: 0.0719096\tbest: 0.0719096 (100)\ttotal: 16s\tremaining: 31.6s\n",
      "125:\tlearn: 0.0537518\ttest: 0.0553283\tbest: 0.0553283 (125)\ttotal: 20.5s\tremaining: 28.2s\n",
      "150:\tlearn: 0.0460188\ttest: 0.0478177\tbest: 0.0478177 (150)\ttotal: 24.8s\tremaining: 24.4s\n",
      "175:\tlearn: 0.0421656\ttest: 0.0441710\tbest: 0.0441710 (175)\ttotal: 29.2s\tremaining: 20.5s\n",
      "200:\tlearn: 0.0401189\ttest: 0.0422748\tbest: 0.0422748 (200)\ttotal: 33.6s\tremaining: 16.5s\n",
      "225:\tlearn: 0.0388543\ttest: 0.0411849\tbest: 0.0411849 (225)\ttotal: 37.9s\tremaining: 12.4s\n",
      "250:\tlearn: 0.0381384\ttest: 0.0405662\tbest: 0.0405662 (250)\ttotal: 42.7s\tremaining: 8.34s\n",
      "275:\tlearn: 0.0376070\ttest: 0.0401371\tbest: 0.0401371 (275)\ttotal: 48s\tremaining: 4.18s\n",
      "299:\tlearn: 0.0373899\ttest: 0.0399587\tbest: 0.0399587 (299)\ttotal: 53s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.03995874791\n",
      "bestIteration = 299\n",
      "\n",
      "0.9915699450213806\n",
      "0.9832808335352556\n"
     ]
    }
   ],
   "source": [
    "model = CatBoostClassifier(loss_function=\"MultiClass\", custom_metric='F1', iterations=300, learning_rate=None, metric_period=25, cat_features=list(range(12)))\n",
    "model.fit(X_tr, y_tr, use_best_model=True, eval_set=(X_te, y_te))\n",
    "y_pred = model.predict(X_te).reshape(-1,)\n",
    "print(f1_score(np.ones_like(y_te), y_te == y_pred))\n",
    "print(np.mean(y_te == y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You should provide test set for use best model. use_best_model parameter has been switched to false value.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.0413101\ttotal: 711ms\tremaining: 3m 32s\n",
      "25:\tlearn: 0.3804145\ttotal: 10.7s\tremaining: 1m 53s\n",
      "50:\tlearn: 0.1798110\ttotal: 20.7s\tremaining: 1m 41s\n",
      "75:\tlearn: 0.0982081\ttotal: 30.7s\tremaining: 1m 30s\n",
      "100:\tlearn: 0.0621114\ttotal: 40.5s\tremaining: 1m 19s\n",
      "125:\tlearn: 0.0450945\ttotal: 50.5s\tremaining: 1m 9s\n",
      "150:\tlearn: 0.0373740\ttotal: 1m\tremaining: 59.4s\n",
      "175:\tlearn: 0.0335899\ttotal: 1m 10s\tremaining: 49.6s\n",
      "200:\tlearn: 0.0316011\ttotal: 1m 21s\tremaining: 40s\n",
      "225:\tlearn: 0.0305871\ttotal: 1m 31s\tremaining: 30.1s\n",
      "250:\tlearn: 0.0300819\ttotal: 1m 42s\tremaining: 20s\n",
      "275:\tlearn: 0.0297303\ttotal: 1m 52s\tremaining: 9.81s\n",
      "299:\tlearn: 0.0295393\ttotal: 2m 1s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x1a64de7c18>"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_model = CatBoostClassifier(loss_function=\"MultiClass\", custom_metric='F1', iterations=300, learning_rate=None, metric_period=25, cat_features=list(range(12)))\n",
    "full_model.fit(coded_X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.88294031e-02, 0.00000000e+00, 3.41596841e-04, 1.61856035e-02,\n",
       "       6.53826534e+00, 2.03638618e-02, 1.22631495e-01, 1.22622637e-01,\n",
       "       1.32238144e-02, 5.17217542e-02, 4.24678952e-08, 2.00711216e-02,\n",
       "       4.84186616e-03, 1.27338514e+01, 2.67178954e+01, 2.76523120e+01,\n",
       "       1.00522346e+01, 4.54821081e+00, 1.13563973e+01])"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_feature_importance()\n",
    "# 4, 13, 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading https://nlp.h-its.org/bpemb/ru/ru.wiki.bpe.vs10000.d50.w2v.bin.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1935516/1935516 [00:00<00:00, 6058636.32B/s]\n"
     ]
    }
   ],
   "source": [
    "bpemb_en = BPEmb(lang=\"ru\", dim=50, vs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "RANDOM_SEED = 997\n",
    "random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    def __init__(self):\n",
    "        self.pos_dict = {}\n",
    "        self.pos_num = {}\n",
    "        self.num_pos = {}\n",
    "        self.max_token_len = 20\n",
    "        self.chars = set()\n",
    "        self.poses = set()\n",
    "        self.model = Sequential()\n",
    "\n",
    "    def prepare_data(self, df):\n",
    "        token_pos = df['token_pos'].values\n",
    "        token_text = df['token_text'].values\n",
    "        k = 0\n",
    "        for i in range(len(token_pos)):\n",
    "            if token_pos[i] not in self.pos_num.keys():\n",
    "                self.pos_num[token_pos[i]] = k\n",
    "                self.num_pos[k] = token_pos[i]\n",
    "                k += 1\n",
    "            self.chars = self.chars.union(set(token_text[i]))\n",
    "            token_text[i] = self.word_norm(token_text[i])\n",
    "            if token_text[i] not in self.pos_dict:\n",
    "                self.pos_dict[token_text[i]] = {}\n",
    "            self.pos_dict[token_text[i]] = token_pos[i]\n",
    "            token_pos[i] = self.pos_num[token_pos[i]]\n",
    "        self.chars.add(' ')\n",
    "        self.chars = sorted(list(self.chars))\n",
    "        #print('total chars:', len(self.chars))\n",
    "        for i in range(len(token_text)):\n",
    "            if len(token_text[i]) <= self.max_token_len:\n",
    "                token_text[i] = (' ' * (self.max_token_len - len(token_text[i]))) + token_text[i]\n",
    "            else:\n",
    "                token_text[i] = token_text[i][:self.max_token_len]\n",
    "        return token_text, token_pos\n",
    "\n",
    "    def vect_tokens(self, tokens):\n",
    "        char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        x = np.zeros((len(tokens), self.max_token_len, len(self.chars)), dtype=np.bool)\n",
    "        for i, token in enumerate(tokens):\n",
    "            for t, char in enumerate(token):\n",
    "                if char in char_indices.keys():\n",
    "                    x[i, t, char_indices[char]] = 1\n",
    "        return x\n",
    "\n",
    "    def vect_poss(self, poss):\n",
    "        y = np.zeros((len(poss), len(self.pos_num)), dtype=np.bool)\n",
    "        for i, pos in enumerate(poss):\n",
    "            y[i, pos] = 1\n",
    "        return y\n",
    "\n",
    "    def word_norm(self, word):\n",
    "        word = word.lower().strip()\n",
    "        return word\n",
    "\n",
    "    def build_model(self, tokens, poss):\n",
    "        self.model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=False, \n",
    "            input_shape=(self.max_token_len, len(self.chars))))\n",
    "            #input_shape=(None, len(self.chars))))\n",
    "        #self.model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "        self.model.add(Dense(100, activation='sigmoid'))\n",
    "        self.model.add(Dropout(0.3))\n",
    "        self.model.add(Dense(len(self.pos_num)))\n",
    "        self.model.add(Activation('softmax'))\n",
    "\n",
    "        optimizer = RMSprop(lr=0.01) #0.01\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    def fit(self, df):\n",
    "        tokens, poss = self.prepare_data(df)\n",
    "        X = self.vect_tokens(tokens)\n",
    "        y = self.vect_poss(poss)\n",
    "        self.build_model(tokens, poss)\n",
    "        self.model.fit(X, y, batch_size=128, epochs=50)\n",
    "        return self\n",
    "\n",
    "    def predict(self, df):\n",
    "        token_text = df['token_text'].values\n",
    "        for i in range(len(token_text)):\n",
    "            if len(token_text[i]) <= self.max_token_len:\n",
    "                token_text[i] = (' ' * (self.max_token_len - len(token_text[i]))) + token_text[i]\n",
    "            else:\n",
    "                token_text[i] = token_text[i][:self.max_token_len]\n",
    "        X = self.vect_tokens(token_text)\n",
    "        y_pred = np.argmax(self.model.predict(X, verbose=0), axis=1)\n",
    "        y_p = []\n",
    "        k = 0\n",
    "        base = True\n",
    "        for i in range(len(y_pred)):\n",
    "            if base:\n",
    "                word = self.word_norm(token_text[i])\n",
    "                if word in self.pos_dict:\n",
    "                    y_p.append(self.pos_dict[word])\n",
    "                    k += 1\n",
    "                else:\n",
    "                    y_p.append(self.num_pos[y_pred[i]])\n",
    "            else:\n",
    "                y_p.append(self.num_pos[y_pred[i]])\n",
    "        y_p = np.array(y_p)\n",
    "        return y_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(X_test, pred):\n",
    "    y_true = X_test['token_pos']\n",
    "    return np.sum(pred == y_true) / len(pred)\n",
    "\n",
    "def test():\n",
    "    df_learn = pd.read_csv(\"./train.csv\", encoding='utf-8')\n",
    "    #print(df_learn.loc[[27262], 'token_text'].values[0] == '?')\n",
    "    X_train, X_test = train_test_split(df_learn, pr=0.7, random_state=RANDOM_SEED)\n",
    "    #p = Pipeline().fit(X_train)\n",
    "    p = Pipeline().fit(X_train)\n",
    "    pred = p.predict(X_test)\n",
    "    print(eval(X_test, pred))\n",
    "\n",
    "def main():\n",
    "    df_learn = pd.read_csv(\"./train.csv\")\n",
    "    p = Pipeline().fit(df_learn)\n",
    "\n",
    "    df_test = pd.read_csv(\"./test.csv\")\n",
    "    y_pred = p.predict(df_test)\n",
    "    solution_name, _ext = os.path.splitext(os.path.basename(__file__))\n",
    "    pd.DataFrame({\"token_pos\": y_pred, \"index\": df_test[\"index\"]}) \\\n",
    "        .to_csv(solution_name + \".csv\", index=False, header=True)\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
